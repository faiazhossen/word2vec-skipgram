{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca232313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f776d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(): \n",
    "    def __init__(self, wordSettings): \n",
    "        self.n = wordSettings['n']  #size of hidden layer(dimension of word embedding)\n",
    "        self.windowSize = wordSettings['windowSize']\n",
    "        self.epochs = wordSettings['epochs']\n",
    "        self.learningRate = wordSettings['learningRate']\n",
    "    \n",
    "    def generateTrainingData(self, wordSettings, corpus): \n",
    "        wordCount = defaultdict(int)\n",
    "        \n",
    "        for row in corpus:\n",
    "            for word in row: \n",
    "                wordCount[word] += 1\n",
    "        \n",
    "#         totalWords = sum([freq**(3/4) for freq in wordCount.values()])\n",
    "#         wordProb = {word:(freq/totalWords)**(3/4) for word, freq in wordCount.items()}\n",
    "        \n",
    "#         wordProb = sorted(wordProb.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "#         print(wordProb)\n",
    "        \n",
    "        self.vocabCount = len(wordCount.keys())  #length of the vocabulary\n",
    "        self.wordList = list(wordCount.keys())   #list of words\n",
    "        self.wordIndex = dict((word, i) for i, word in enumerate(self.wordList))  #list of word index\n",
    "        self.indexWord = dict((i, word) for i, word in enumerate(self.wordList))  #list of index word\n",
    "        \n",
    "        trainingData = []   # for each target word, it will hold all the context words\n",
    "        \n",
    "        for sentence in corpus: \n",
    "            sentenceLength = len(sentence)\n",
    "            \n",
    "            for targetWordIndex, word in enumerate(sentence):\n",
    "                \n",
    "\n",
    "                wordContext = []\n",
    "                \n",
    "                for contextWordIndex in range(targetWordIndex - self.windowSize, targetWordIndex + self.windowSize+1):\n",
    "                    if contextWordIndex != targetWordIndex and contextWordIndex <= sentenceLength - 1 and contextWordIndex >=0:\n",
    "                        wordContext.append(contextWordIndex)\n",
    "                trainingData.append([targetWordIndex, wordContext])\n",
    "                \n",
    "        return np.array(trainingData)\n",
    "\n",
    "    \n",
    "    def findWordindex(self, word): \n",
    "        return self.wordIndex[word]\n",
    "    \n",
    "    def train(self, trainingData): \n",
    "        self.weightToHidden = np.random.uniform(-1,1, (self.vocabCount+1, self.n))\n",
    "        self.weightToOutput = np.random.uniform(-1,1, (self.n, self.vocabCount+1))\n",
    "\n",
    "        for i in range(self.epochs): \n",
    "            self.loss = 0\n",
    "            \n",
    "            for wordTarget, wordContext in trainingData: \n",
    "                error = np.zeros(self.vocabCount+1)\n",
    "                hiddenMat = self.weightToHidden[wordTarget,:]\n",
    "                w_c = []\n",
    "                randomSample = []\n",
    "                for wc in wordContext: \n",
    "                    randomSample = self.generateRandomSample(wordContext, wordTarget)\n",
    "                    randomSample.append(wc)\n",
    "                    randomSample = np.sort(randomSample)\n",
    "                    \n",
    "                    wt = np.zeros(self.vocabCount + 1)\n",
    "                    wt[wc] = 1\n",
    "                    w_c.append(wt)    #should have used one hot vector instead of index while generating training data\n",
    "                    \n",
    "                outputMatrix = np.zeros(self.vocabCount+1)\n",
    "                    \n",
    "                for w in randomSample:\n",
    "                    outputMatrix[w] = self.sigmoid(np.dot(hiddenMat , self.weightToOutput[:,w]))\n",
    "                \n",
    "            error = np.sum([np.subtract(outputMatrix, word) for word in w_c], axis=0)\n",
    "            self.backpropagate(error, hiddenMat, wt)\n",
    "#         print(self.weightToHidden)\n",
    "                            \n",
    "    \n",
    "    def backpropagate(self, error, hiddenMat, wordTarget):\n",
    "        for i,e in enumerate(error):\n",
    "            self.weightToHidden[i,:] -= e * self.learningRate\n",
    "            self.weightToOutput[:,i] -= e * self.learningRate\n",
    "                            \n",
    "                        \n",
    "                        \n",
    "    \n",
    "    def generateRandomSample(self, wordContext, wt):\n",
    "        count = 0\n",
    "        randSample = []\n",
    "        while count < 3: \n",
    "            index = random.randint(0, (self.vocabCount))\n",
    "            if index != wt and index not in wordContext and index not in randSample: \n",
    "                randSample.append(index)\n",
    "                count += 1\n",
    "        return randSample\n",
    "    \n",
    "\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        try:\n",
    "            return 1 / (1 + math.exp(-x))\n",
    "        except OverflowError:\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "    def printWordVector(self, word): \n",
    "        wordIndex = self.wordIndex[word]\n",
    "        print(word, self.weightToHidden[wordIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0c87863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 list([1, 2])]\n",
      " [1 list([0, 2, 3])]\n",
      " [2 list([0, 1, 3, 4])]\n",
      " [3 list([1, 2, 4, 5])]\n",
      " [4 list([2, 3, 5, 6])]\n",
      " [5 list([3, 4, 6, 7])]\n",
      " [6 list([4, 5, 7, 8])]\n",
      " [7 list([5, 6, 8, 9])]\n",
      " [8 list([6, 7, 9])]\n",
      " [9 list([7, 8])]]\n",
      "machine [-1.08555227  0.35426376 -1.20919577 -0.70604796 -0.78444256  0.21721254\n",
      "  0.19047188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-118-481e0ace6b72>:42: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(trainingData)\n"
     ]
    }
   ],
   "source": [
    "settings = {\n",
    "    'windowSize': 2,\n",
    "    'n': 7,                    # dimensions of word embeddings, also refer to size of hidden layer\n",
    "    'epochs': 50,               # number of training epochs\n",
    "    'learningRate': 0.1        # learning rate\n",
    "}\n",
    "\n",
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "\n",
    "\n",
    "w2v = Word2Vec(settings)\n",
    "\n",
    "trainingData = w2v.generateTrainingData(settings, corpus)\n",
    "w2v.train(trainingData)\n",
    "print(trainingData)\n",
    "w2v.printWordVector('machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4767ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d52c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
